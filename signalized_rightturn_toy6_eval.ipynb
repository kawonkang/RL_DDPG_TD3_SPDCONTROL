{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67571c39",
   "metadata": {},
   "source": [
    "# 우회전 유스케이스 분석\n",
    "\n",
    "![description](imgs/image_RT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2c8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태에 min_spacing등만 넣을게 아니라, min2_spacing 등 추가\n",
    "# state 변경 13개\n",
    "#     \"\"\"\n",
    "#     State Vector (예시: 13차원)\n",
    "#     0:  ego_speed           (0~1, 50 km/h 기준 정규화)\n",
    "#     1:  dtc_norm            (0~1, 충돌지점까지 남은 거리 / 200 m)\n",
    "#     2:  time_to_Gn_norm     (0~1, GN까지 남은 시간 / 100 s)\n",
    "#     3:  f1_spacing_norm     (0~1, 가장 가까운 following spacing)\n",
    "#     4:  f1_ttc_norm         (0~1, 가장 가까운 following TTC)\n",
    "#     5:  f1_rel_speed_norm   (~[-1,1], 가장 가까운 following 상대속도)\n",
    "#     6:  f2_spacing_norm     (0~1, 두 번째 following spacing)\n",
    "#     7:  f2_ttc_norm         (0~1, 두 번째 following TTC)\n",
    "#     8:  c1_spacing_norm     (0~1, 가장 가까운 crossing spacing)\n",
    "#     9:  c1_pet_norm         (0~1, 가장 가까운 crossing PET)\n",
    "#     10: c2_spacing_norm     (0~1, 두 번째 crossing spacing)\n",
    "#     11: c2_pet_norm         (0~1, 두 번째 crossing PET)\n",
    "#     12: c1_rel_speed_norm   (~[-1,1], 가장 가까운 crossing 상대속도)\n",
    "#     \"\"\"\n",
    "    \n",
    "# 앞뒤차 4대\n",
    "# 0~70 km/h 범위 유지\n",
    "# 가감속 범위 -4.5~3.0 m/s²\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ae00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VISSIM TD3 Template for Single-Ego Accel Control at Signalized Intersection\n",
    "- Author: Kawon Kang\n",
    "- Purpose: Single vehicle (ego) continuous accel guidance via TD3\n",
    "- Control: action a_t [m/s^2] -> target desired speed (DesSpeed) for ego\n",
    "- Safety layer + yellow dilemma rule as post-processing on action\n",
    "\"\"\"\n",
    "\n",
    "# basic libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple, Optional\n",
    "\n",
    "# VISSIM COM\n",
    "import win32com.client as COM\n",
    "\n",
    "# data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PyTorch 기반 TD3\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "# 재현성\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c621a5",
   "metadata": {},
   "source": [
    "PPO, DDPG, TD3 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "184118d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# VISSIM 유틸 (원본 최대 유지)\n",
    "# =========================\n",
    "def is_vehicle_valid(vehicle):\n",
    "    try:\n",
    "        vehicle.AttValue('No')\n",
    "        vehicle.AttValue('Speed')\n",
    "        vehicle.AttValue('Pos')\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_valid_vehicles(Vissim):\n",
    "    try:\n",
    "        vehicles = []\n",
    "        for vehicle in Vissim.Net.Vehicles:\n",
    "            try:\n",
    "                vehicle_no = vehicle.AttValue('No')\n",
    "                vehicle_pos = vehicle.AttValue('Pos')\n",
    "                vehicle_speed = vehicle.AttValue('Speed')\n",
    "                vehicle_link = vehicle.AttValue('Lane')\n",
    "                if all(attr is not None for attr in [vehicle_no, vehicle_pos, vehicle_speed, vehicle_link]):\n",
    "                    vehicles.append(vehicle)\n",
    "            except:\n",
    "                continue\n",
    "        return vehicles\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_valid_vehicles: {str(e)}\")\n",
    "        return []\n",
    "def reset_network(Vissim):\n",
    "        # try:\n",
    "    Vissim.Simulation.Stop()\n",
    "    try:\n",
    "        for vehicle in list(Vissim.Net.Vehicles):\n",
    "            try:\n",
    "                Vissim.Net.Vehicles.RemoveVehicle(vehicle.AttValue('No'))\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "def create_vehicles(Vissim, link_ranges):\n",
    "    # try:\n",
    "    Vissim.Simulation.Stop()\n",
    "    try:\n",
    "        for vehicle in list(Vissim.Net.Vehicles):\n",
    "            try:\n",
    "                Vissim.Net.Vehicles.RemoveVehicle(vehicle.AttValue('No'))\n",
    "            except:\n",
    "                continue\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(0.2)\n",
    "    cv_rand = np.random.randint(1, 100)\n",
    "    Vissim.Simulation.SetAttValue('RandSeed', cv_rand)\n",
    "    Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "    for _ in range(np.random.randint(85,100)): # 80~ 100초까지 warm-up 100초에 남직신호 시작\n",
    "        Vissim.Simulation.RunSingleStep()\n",
    "    \n",
    "    created_vehicles = []\n",
    "    regular_vehicles = [\n",
    "        (11, link_ranges[1][0], link_ranges[1][1], 1), # link, start_pos, end_pos, veh_num\n",
    "        (11, link_ranges[2][0], link_ranges[2][1], 2), \n",
    "        (11, link_ranges[3][0], link_ranges[3][1], 3),\n",
    "        (11, link_ranges[4][0], link_ranges[4][1], 4)\n",
    "    ]\n",
    "    for link, start_pos, end_pos, veh_num in regular_vehicles:\n",
    "        vehicle = Vissim.Net.Vehicles.AddVehicleAtLinkPosition( #주변 차량 생성\n",
    "            100, link, (veh_num+1)//2 , np.random.uniform(start_pos, end_pos), 40, True #Type, Lane, Pos, DesSpeed, Interaction(Opt)-> 주변객체 상호작용 여부\n",
    "        )\n",
    "        if vehicle:\n",
    "            vehicle.SetAttValue('DesSpeed', 40) # 물어보기\n",
    "            # vehicle.SetAttValue('DesLane', random.choice([1,2])) # 조정필요\n",
    "            created_vehicles.append(vehicle)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    ego_pos = np.random.uniform(50, 60) # 조정필요\n",
    "    ego_vehicle = Vissim.Net.Vehicles.AddVehicleAtLinkPosition(\n",
    "        630, 11, np.random.choice([1,2]), ego_pos, 50, True \n",
    "    )\n",
    "    if ego_vehicle:\n",
    "        ego_vehicle.SetAttValue('DesSpeed', 50)\n",
    "        ego_vehicle.SetAttValue('DesLane', 1) # 우회전만 하라.\n",
    "        created_vehicles.append(ego_vehicle)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # follower_pos = ego_pos - 30\n",
    "    # if follower_pos >= link_ranges[3][0]:\n",
    "    #     follower = Vissim.Net.Vehicles.AddVehicleAtLinkPosition(\n",
    "    #         100, 3, 1, follower_pos, 40, True \n",
    "    #     )\n",
    "    #     if follower:\n",
    "    #         follower.SetAttValue('DesSpeed', 50)\n",
    "    #         created_vehicles.append(follower)\n",
    "\n",
    "    if len(created_vehicles) > 3:\n",
    "        time.sleep(0.2)\n",
    "        Vissim.Simulation.RunSingleStep()\n",
    "        return created_vehicles\n",
    "    return None\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error in create_vehicles: {str(e)}\")\n",
    "    #     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ce4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LINK_LENGTHS = {\n",
    "    \"11-1\": 149.4,\n",
    "    \"11-2\": 149.4,\n",
    "    \"10002-1\": 39.76,\n",
    "    \"4-1\": 18.22,\n",
    "    \"9-1\": 160.03,\n",
    "    \"10009-1\": 40.89,\n",
    "    \"7-1\": 200.0  # 필요 시 실제 길이로 수정\n",
    "}\n",
    "\n",
    "def get_dist_to_conflict(link_id, pos, role=\"ego\"):\n",
    "    \"\"\"상충 지점까지의 남은 거리(DTC) 계산\"\"\"\n",
    "    dtc = 1000.0\n",
    "    try:\n",
    "        link_id = str(link_id)\n",
    "        if role == \"ego\":\n",
    "            if link_id in [\"11-1\", \"11-2\"]:\n",
    "                dtc = (LINK_LENGTHS[\"11-1\"] - pos) + LINK_LENGTHS[\"10002-1\"] + LINK_LENGTHS[\"4-1\"]\n",
    "            elif link_id == \"10002-1\":\n",
    "                dtc = (LINK_LENGTHS[\"10002-1\"] - pos) + LINK_LENGTHS[\"4-1\"]\n",
    "            elif link_id == \"4-1\":\n",
    "                dtc = (LINK_LENGTHS[\"4-1\"] - pos)\n",
    "            elif link_id == \"7-1\":\n",
    "                dtc = -pos\n",
    "        elif role == \"other\":\n",
    "            if link_id == \"9-1\":\n",
    "                dtc = (LINK_LENGTHS[\"9-1\"] - pos) + LINK_LENGTHS[\"10009-1\"]\n",
    "            elif link_id == \"10009-1\":\n",
    "                dtc = (LINK_LENGTHS[\"10009-1\"] - pos)\n",
    "            elif link_id == \"7-1\":\n",
    "                dtc = -pos\n",
    "    except:\n",
    "        pass\n",
    "    return dtc\n",
    "\n",
    "def calculate_observations(vehicle_list, ego_num=5):\n",
    "    \"\"\"\n",
    "    모든 차량에 대해 Spacing과 시간지표(TTC/PET)를 계산.\n",
    "    obs_dict[key] = {\n",
    "        'type': 'following' or 'crossing',\n",
    "        'spacing': ...,\n",
    "        'ttc': ... (following),\n",
    "        'pet': ... (crossing),\n",
    "        'rel_speed': ... (공통, m/s)\n",
    "    }\n",
    "    \"\"\"\n",
    "    obs_dict = {}\n",
    "    ego_vehicle = None\n",
    "    \n",
    "    # Ego 찾기\n",
    "    for vehicle in vehicle_list:\n",
    "        if vehicle.AttValue('No') == ego_num:\n",
    "            ego_vehicle = vehicle\n",
    "            break\n",
    "    if not ego_vehicle:\n",
    "        return obs_dict\n",
    "\n",
    "    ego_link = str(ego_vehicle.AttValue('Lane'))              # \"11-1\", \"7-1\" 이런 구조\n",
    "    ego_pos  = float(ego_vehicle.AttValue('Pos'))\n",
    "    ego_speed = float(ego_vehicle.AttValue('Speed') or 0) / 3.6   # m/s\n",
    "    dtc_ego = get_dist_to_conflict(ego_link, ego_pos, role=\"ego\")\n",
    "\n",
    "    crossing_target_links = [\"9-1\", \"10009-1\"]\n",
    "\n",
    "    for other_vehicle in vehicle_list:\n",
    "        oid = other_vehicle.AttValue('No')\n",
    "        if oid == ego_num:\n",
    "            continue\n",
    "\n",
    "        other_link = str(other_vehicle.AttValue('Lane'))\n",
    "        other_pos = float(other_vehicle.AttValue('Pos'))\n",
    "        other_speed = float(other_vehicle.AttValue('Speed') or 0) / 3.6  # m/s\n",
    "        \n",
    "        # 1) 같은 차선: following\n",
    "        if ego_link == other_link and other_pos > ego_pos:\n",
    "            spacing = other_pos - ego_pos\n",
    "            rel_speed = ego_speed - other_speed          # ego 기준 closing speed\n",
    "            ttc = spacing / (rel_speed + 1e-6) if rel_speed > 0 else 100.0\n",
    "\n",
    "            if spacing < 60:\n",
    "                obs_dict[f\"{ego_num}_{oid}\"] = {\n",
    "                    \"type\": \"following\",\n",
    "                    \"spacing\": spacing,\n",
    "                    \"ttc\": ttc,\n",
    "                    \"rel_speed\": rel_speed\n",
    "                }\n",
    "\n",
    "        # 2) 교차 차선: crossing\n",
    "        elif (other_link in crossing_target_links) and (dtc_ego > -5):\n",
    "            dtc_other = get_dist_to_conflict(other_link, other_pos, role=\"other\")\n",
    "            if dtc_other > -5:\n",
    "                spacing = max(0, dtc_ego) + max(0, dtc_other)\n",
    "                tta_ego = dtc_ego / max(ego_speed, 0.1)\n",
    "                tta_other = dtc_other / max(other_speed, 0.1)\n",
    "                pet = abs(tta_ego - tta_other)\n",
    "\n",
    "                # 교차 상황에서도 상대속도 정의 (단순 ego_speed - other_speed)\n",
    "                rel_speed = ego_speed - other_speed\n",
    "\n",
    "                if tta_ego < 20:\n",
    "                    obs_dict[f\"{ego_num}_{oid}\"] = {\n",
    "                        \"type\": \"crossing\",\n",
    "                        \"spacing\": spacing,\n",
    "                        \"pet\": pet,\n",
    "                        \"tta_ego\": tta_ego,\n",
    "                        \"tta_other\": tta_other,\n",
    "                        \"rel_speed\": rel_speed      # ★ 여기 추가\n",
    "                    }\n",
    "\n",
    "    return obs_dict\n",
    "\n",
    "\n",
    "def get_reward(ego_vehicle, obs_dict):\n",
    "    if ego_vehicle is None: return 0.0, False\n",
    "    \n",
    "    reward = 1 # 생존 보상\n",
    "    done = False\n",
    "    \n",
    "    # 가장 위험한 값 추출 (Spacing도 보고, 시간도 봄)\n",
    "    min_spacing = 1000.0\n",
    "    min_time_metric = 1000.0 # TTC or PET\n",
    "    \n",
    "    try:\n",
    "        ego_speed = float(ego_vehicle.AttValue('Speed') or 0.0)\n",
    "        ego_link  = str(ego_vehicle.AttValue('Lane'))   # \"7-1\" 구조\n",
    "        ego_pos   = float(ego_vehicle.AttValue('Pos') or 0.0)\n",
    "\n",
    "        # ----- 1) 거리 / 시간 기반 페널티 -----\n",
    "        min_spacing = 1000.0\n",
    "        min_time_metric = 1000.0\n",
    "\n",
    "        for _, data in obs_dict.items():\n",
    "            if data[\"spacing\"] < min_spacing:\n",
    "                min_spacing = data[\"spacing\"]\n",
    "            if data[\"type\"] == \"following\":\n",
    "                if data[\"ttc\"] < min_time_metric:\n",
    "                    min_time_metric = data[\"ttc\"]\n",
    "            elif data[\"type\"] == \"crossing\":\n",
    "                if data[\"pet\"] < min_time_metric:\n",
    "                    min_time_metric = data[\"pet\"]\n",
    "        # --------------------------------------------------------\n",
    "        # [보상 로직] Spacing과 Time Metric을 모두 사용\n",
    "        # --------------------------------------------------------\n",
    "        \n",
    "        # 1. Spacing 기반 (물리적 충돌 방지 - 최우선)\n",
    "        if min_spacing < 2.0: # 충돌\n",
    "            reward -= 200\n",
    "            return float(reward), True # 종료\n",
    "        elif min_spacing < 5.0: # 너무 가까움\n",
    "            reward -= 10\n",
    "        elif min_spacing < 10.0: # 주의\n",
    "            reward -= 2\n",
    "\n",
    "        # 2. Time 기반 (예측 위험 회피)\n",
    "        # 거리가 좀 있어도(20m), 속도가 빨라서 TTC/PET가 1초면 위험한 상황\n",
    "        if min_time_metric < 1.5: # 급위험\n",
    "            reward -= 10\n",
    "        elif min_time_metric < 3.0: # 주의\n",
    "            reward -= 2\n",
    "\n",
    "        # 3. 속도 보상 (추가)\n",
    "        ego_speed = float(ego_vehicle.AttValue('Speed') or 0)\n",
    "        if 30 <= ego_speed <= 50:\n",
    "            reward += 1.0\n",
    "\n",
    "        # 4. 목표 도달 보상 (추가)\n",
    "        # 성공 조건: 7-1 진입 후 일정 위치 이상 (예: 40m)\n",
    "        if ego_link == \"7-1\" and ego_pos >= 40.0:\n",
    "            reward += 50.0     # 성공 보상\n",
    "            done = True        # 에피소드 종료 플래그\n",
    "            return float(reward), done\n",
    "\n",
    "        return float(reward), done\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_reward: {str(e)}\")\n",
    "        return 0.0, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6916580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(vehicles, obs_dict, ego_num, simsec):\n",
    "    \"\"\"\n",
    "    State Vector (예시: 13차원)\n",
    "    0:  ego_speed           (0~1, 50 km/h 기준 정규화)\n",
    "    1:  dtc_norm            (0~1, 충돌지점까지 남은 거리 / 200 m)\n",
    "    2:  time_to_Gn_norm     (0~1, GN까지 남은 시간 / 100 s)\n",
    "    3:  f1_spacing_norm     (0~1, 가장 가까운 following spacing)\n",
    "    4:  f1_ttc_norm         (0~1, 가장 가까운 following TTC)\n",
    "    5:  f1_rel_speed_norm   (~[-1,1], 가장 가까운 following 상대속도)\n",
    "    6:  f2_spacing_norm     (0~1, 두 번째 following spacing)\n",
    "    7:  f2_ttc_norm         (0~1, 두 번째 following TTC)\n",
    "    8:  c1_spacing_norm     (0~1, 가장 가까운 crossing spacing)\n",
    "    9:  c1_pet_norm         (0~1, 가장 가까운 crossing PET)\n",
    "    10: c2_spacing_norm     (0~1, 두 번째 crossing spacing)\n",
    "    11: c2_pet_norm         (0~1, 두 번째 crossing PET)\n",
    "    12: c1_rel_speed_norm   (~[-1,1], crossing 상대속도)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ego_vehicle = next((v for v in vehicles if v.AttValue('No') == ego_num), None)\n",
    "        if not ego_vehicle:\n",
    "            # state_dim과 길이 맞춰서 리턴\n",
    "            return np.zeros(13, dtype=np.float32)\n",
    "\n",
    "        # 1. Ego 정보\n",
    "        ego_speed = float(ego_vehicle.AttValue('Speed') or 0) / 50.0  # 50 km/h 기준\n",
    "        ego_link  = ego_vehicle.AttValue('Lane')                # 링크 ID\n",
    "        ego_pos   = float(ego_vehicle.AttValue('Pos') or 0)\n",
    "\n",
    "        # DTC (충돌지점까지 남은 거리)\n",
    "        dtc_ego = get_dist_to_conflict(ego_link, ego_pos, role=\"ego\")\n",
    "        dtc_norm = min(max(dtc_ego, 0.0) / 200.0, 1.0)                # 0~200 m 기준\n",
    "\n",
    "        # 2. 신호까지 남은 시간 (예: 0~100초)\n",
    "        time_to_Gn = max(0.0, 100.0 - float(simsec))                  # 임시 예시\n",
    "        time_to_Gn_norm = min(time_to_Gn / 100.0, 1.0)\n",
    "\n",
    "        # 3. Following / Crossing 리스트 생성\n",
    "        following_list = []   # [{'spacing_norm':..., 'ttc_norm':..., 'rel_speed_norm':...}, ...]\n",
    "        crossing_list  = []   # [{'spacing_norm':..., 'pet_norm':...}, ...]\n",
    "\n",
    "        if obs_dict:\n",
    "            for _, data in obs_dict.items():\n",
    "                if data['type'] == 'following':\n",
    "                    spacing_norm = min(data['spacing'] / 100.0, 1.0)\n",
    "                    ttc_norm     = min(data['ttc']     / 20.0, 1.0)\n",
    "                    rel_speed_norm = (data.get('rel_speed', 0.0) / 50.0)  # km/h 기준이면 /50\n",
    "\n",
    "                    following_list.append({\n",
    "                        'spacing_norm': spacing_norm,\n",
    "                        'ttc_norm': ttc_norm,\n",
    "                        'rel_speed_norm': rel_speed_norm\n",
    "                    })\n",
    "\n",
    "                elif data['type'] == 'crossing':\n",
    "                    spacing_norm = min(data['spacing'] / 100.0, 1.0)\n",
    "                    pet_norm     = min(data['pet']     / 20.0, 1.0)\n",
    "                    rel_speed_norm = (data.get('rel_speed', 0.0) / 50.0)  # km/h 기준이면 /50\n",
    "\n",
    "                    crossing_list.append({\n",
    "                        'spacing_norm': spacing_norm,\n",
    "                        'pet_norm': pet_norm,\n",
    "                        'rel_speed_norm': rel_speed_norm\n",
    "                    })\n",
    "\n",
    "        # 4. spacing 기준으로 정렬 후 상위 2개만 사용, 부족하면 패딩\n",
    "        following_list.sort(key=lambda x: x['spacing_norm'])\n",
    "        crossing_list.sort(key=lambda x: x['spacing_norm'])\n",
    "\n",
    "        follow_default = {'spacing_norm': 1.0, 'ttc_norm': 1.0, 'rel_speed_norm': 0.0}\n",
    "        cross_default  = {'spacing_norm': 1.0, 'pet_norm': 1.0, 'rel_speed_norm': 0.0}\n",
    "\n",
    "        while len(following_list) < 2:\n",
    "            following_list.append(follow_default)\n",
    "        while len(crossing_list) < 2:\n",
    "            crossing_list.append(cross_default)\n",
    "\n",
    "        f1, f2 = following_list[0], following_list[1]\n",
    "        c1, c2 = crossing_list[0], crossing_list[1]\n",
    "\n",
    "        \n",
    "        state = np.array([\n",
    "            ego_speed,\n",
    "            dtc_norm,\n",
    "            time_to_Gn_norm,\n",
    "            f1['spacing_norm'], # 가장 가까운 following\n",
    "            f1['ttc_norm'],\n",
    "            f1['rel_speed_norm'], # 가장 가까운 following 상대속도\n",
    "            f2['spacing_norm'], # 두 번째 following\n",
    "            f2['ttc_norm'],\n",
    "            c1['spacing_norm'], # 가장 가까운 crossing\n",
    "            c1['pet_norm'],\n",
    "            c1['rel_speed_norm'], # 가장 가까운 crossing 상대속도\n",
    "            c2['spacing_norm'], # 두 번째 crossing\n",
    "            c2['pet_norm']\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        return state\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_state: {str(e)}\")\n",
    "        return np.zeros(13, dtype=np.float32)\n",
    "\n",
    "def apply_action(vehicle_list, Vissim, acceleration):\n",
    "    try:\n",
    "        for vehicle in vehicle_list:\n",
    "            if vehicle.AttValue('VehType') == '630':\n",
    "                current_speed = vehicle.AttValue('Speed')  # km/h\n",
    "                speed_ms = current_speed / 3.6\n",
    "                delta_v = float(acceleration) * 1  # 1초 스텝\n",
    "                new_speed_ms = speed_ms + delta_v\n",
    "                new_speed_ms = max(30/3.6, min(new_speed_ms, 50/3.6))\n",
    "                vehicle.SetAttValue('DesSpeed', new_speed_ms * 3.6)\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying action: {str(e)}\")\n",
    "\n",
    "\n",
    "def is_episode_done(ego_vehicle, obs_dict):\n",
    "    try:\n",
    "        if ego_vehicle is None:\n",
    "            return False\n",
    "        \n",
    "        ego_link = str(ego_vehicle.AttValue('Lane'))\n",
    "        ego_pos  = float(ego_vehicle.AttValue('Pos'))\n",
    "\n",
    "        # 충돌 조건\n",
    "        for _, data in obs_dict.items():\n",
    "            if data['spacing'] < 2.0:\n",
    "                return True\n",
    "\n",
    "        # 비정상 속도\n",
    "        speed = float(ego_vehicle.AttValue('Speed'))\n",
    "        if speed < 0 or speed > 70:\n",
    "            print(\"Abnormal speed detected:\", speed)\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking episode completion: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def Network(Network_Path, inpx_Name, layx_Name):\n",
    "    # try:\n",
    "    Vissim = COM.Dispatch(\"Vissim.Vissim\")\n",
    "    Filename = os.path.join(Network_Path, inpx_Name)\n",
    "    flag_read_additionally = False\n",
    "    Vissim.LoadNet(Filename, flag_read_additionally)\n",
    "    Filename = os.path.join(Network_Path, layx_Name)\n",
    "    Vissim.LoadLayout(Filename)\n",
    "    for simRun in Vissim.Net.SimulationRuns:\n",
    "        Vissim.Net.SimulationRuns.RemoveSimulationRun(simRun)\n",
    "    return Vissim\n",
    "    \n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error initializing Vissim: {str(e)}\")\n",
    "    #     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecbac8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Replay Buffer (TD3용)\n",
    "# =========================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # action: float 가속도\n",
    "        self.buffer.append((np.array(state, dtype=np.float32),\n",
    "                            float(action),\n",
    "                            float(reward),\n",
    "                            np.array(next_state, dtype=np.float32),\n",
    "                            float(done)))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states      = np.array(states, dtype=np.float32)\n",
    "        actions     = np.array(actions, dtype=np.float32).reshape(-1, 1)  # (B,1)\n",
    "        rewards     = np.array(rewards, dtype=np.float32).reshape(-1, 1)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones       = np.array(dones, dtype=np.float32).reshape(-1, 1)\n",
    "        \n",
    "        # 라이트 클립으로 수치 안정화\n",
    "        states      = np.clip(states, -10, 10)\n",
    "        next_states = np.clip(next_states, -10, 10)\n",
    "        rewards     = np.clip(rewards, -100, 100)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TD3 네트워크\n",
    "# =========================\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, act_low=-3.0, act_high=3.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1), nn.Tanh()  # [-1, 1]\n",
    "        )\n",
    "        # 행동 스케일\n",
    "        self.register_buffer(\"act_low\",  torch.tensor([act_low], dtype=torch.float32))\n",
    "        self.register_buffer(\"act_high\", torch.tensor([act_high], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, s):\n",
    "        u = self.net(s)  # [-1,1]\n",
    "        return (self.act_high - self.act_low) * 0.5 * u + (self.act_high + self.act_low) * 0.5\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        def qnet():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(state_dim + 1, 400), nn.ReLU(),\n",
    "                nn.Linear(400, 300), nn.ReLU(),\n",
    "                nn.Linear(300, 1)\n",
    "            )\n",
    "        self.q1 = qnet()\n",
    "        self.q2 = qnet()\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        sa = torch.cat([s, a], dim=-1)\n",
    "        return self.q1(sa), self.q2(sa)\n",
    "\n",
    "    def q1_only(self, s, a):\n",
    "        sa = torch.cat([s, a], dim=-1)\n",
    "        return self.q1(sa)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TD3 Agent (PyTorch)\n",
    "# =========================\n",
    "class TD3Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim=13, # 상태 차원\n",
    "        act_low=-4.5, # 가감속 범위\n",
    "        act_high=3.0,\n",
    "        actor_lr=1.5e-5,\n",
    "        critic_lr=1.5e-4,\n",
    "        gamma=0.99,\n",
    "        tau=0.001,\n",
    "        policy_noise=0.2,\n",
    "        noise_clip=0.5,\n",
    "        policy_delay=2,\n",
    "        batch_size=512,\n",
    "        start_steps=5000,\n",
    "        expl_noise=0.2\n",
    "    ):\n",
    "        self.state_dim   = state_dim\n",
    "        self.act_low     = act_low\n",
    "        self.act_high    = act_high\n",
    "        self.gamma       = gamma\n",
    "        self.tau         = tau\n",
    "        self.policy_noise= policy_noise\n",
    "        self.noise_clip  = noise_clip\n",
    "        self.policy_delay= policy_delay\n",
    "        self.batch_size  = batch_size\n",
    "        self.start_steps = start_steps\n",
    "        self.expl_noise  = expl_noise\n",
    "\n",
    "        self.actor     = Actor(state_dim, act_low, act_high).to(device)\n",
    "        self.actor_t   = Actor(state_dim, act_low, act_high).to(device)\n",
    "        self.critic    = Critic(state_dim).to(device)\n",
    "        self.critic_t  = Critic(state_dim).to(device)\n",
    "\n",
    "        self.actor_t.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_t.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.opt_actor  = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=100000)\n",
    "        self.total_it = 0\n",
    "\n",
    "        # 로깅\n",
    "        self.training_metrics = {\n",
    "            'critic_losses': [],\n",
    "            'actor_losses': [],\n",
    "            'q_values': [],\n",
    "            'rewards': []\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        state: np.ndarray (state_dim,)\n",
    "        return: float acceleration in [act_low, act_high]\n",
    "        \"\"\"\n",
    "        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)  # (1, state_dim)\n",
    "        if training and self.total_it < self.start_steps:\n",
    "            a = np.random.uniform(self.act_low, self.act_high)\n",
    "            return float(a)\n",
    "\n",
    "        a = self.actor(s).cpu().numpy()[0, 0]\n",
    "        if training:\n",
    "            a += np.random.normal(0, self.expl_noise)\n",
    "        return float(np.clip(a, self.act_low, self.act_high))\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.total_it += 1\n",
    "        s, a, r, s2, d = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        s  = torch.tensor(s,  dtype=torch.float32, device=device)\n",
    "        a  = torch.tensor(a,  dtype=torch.float32, device=device)\n",
    "        r  = torch.tensor(r,  dtype=torch.float32, device=device)\n",
    "        s2 = torch.tensor(s2, dtype=torch.float32, device=device)\n",
    "        d  = torch.tensor(d,  dtype=torch.float32, device=device)\n",
    "\n",
    "        # ----- Critic update -----\n",
    "        with torch.no_grad():\n",
    "            a2 = self.actor_t(s2)\n",
    "            noise = (torch.randn_like(a2) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "            a2 = (a2 + noise).clamp(self.act_low, self.act_high)\n",
    "            q1_t, q2_t = self.critic_t(s2, a2)\n",
    "            y = r + self.gamma * (1.0 - d) * torch.min(q1_t, q2_t)\n",
    "\n",
    "        q1, q2 = self.critic(s, a)\n",
    "        critic_loss = (q1 - y).pow(2).mean() + (q2 - y).pow(2).mean()\n",
    "\n",
    "        self.opt_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n",
    "        self.opt_critic.step()\n",
    "\n",
    "        actor_loss = torch.tensor(0.0, device=device)\n",
    "        # ----- Delayed policy update -----\n",
    "        if self.total_it % self.policy_delay == 0:\n",
    "            a_pred = self.actor(s)\n",
    "            actor_loss = -self.critic.q1_only(s, a_pred).mean()\n",
    "            self.opt_actor.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "            self.opt_actor.step()\n",
    "\n",
    "            # Polyak update\n",
    "            with torch.no_grad():\n",
    "                for p, pt in zip(self.actor.parameters(), self.actor_t.parameters()):\n",
    "                    pt.data.mul_(1 - self.tau)\n",
    "                    pt.data.add_(self.tau * p.data)\n",
    "                for p, pt in zip(self.critic.parameters(), self.critic_t.parameters()):\n",
    "                    pt.data.mul_(1 - self.tau)\n",
    "                    pt.data.add_(self.tau * p.data)\n",
    "\n",
    "        # 로깅\n",
    "        with torch.no_grad():\n",
    "            self.training_metrics['critic_losses'].append(float(critic_loss.item()))\n",
    "            self.training_metrics['actor_losses'].append(float(actor_loss.item()) if self.total_it % self.policy_delay == 0 else np.nan)\n",
    "            self.training_metrics['q_values'].append(float(q1.mean().item()))\n",
    "\n",
    "    # 저장/로드 (경량)\n",
    "    def save_models(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(self.actor.state_dict(),  os.path.join(path, \"actor.pt\"))\n",
    "        torch.save(self.critic.state_dict(), os.path.join(path, \"critic.pt\"))\n",
    "        np.save(os.path.join(path, \"train_meta.npy\"), {\"total_it\": self.total_it})\n",
    "\n",
    "    def load_models(self, path):\n",
    "        self.actor.load_state_dict(torch.load(os.path.join(path, \"actor.pt\"), map_location=device))\n",
    "        self.critic.load_state_dict(torch.load(os.path.join(path, \"critic.pt\"), map_location=device))\n",
    "        self.actor_t.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_t.load_state_dict(self.critic.state_dict())\n",
    "        meta_path = os.path.join(path, \"train_meta.npy\")\n",
    "        if os.path.exists(meta_path):\n",
    "            meta = np.load(meta_path, allow_pickle=True).item()\n",
    "            self.total_it = int(meta.get(\"total_it\", 0))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 평가 루틴 (TD3)\n",
    "# =========================\n",
    "def evaluate(Vissim, agent, link_ranges, num_episodes=10):\n",
    "    results = []\n",
    "    for episode in range(num_episodes):\n",
    "        vehicles = None\n",
    "        while vehicles is None:\n",
    "            vehicles = create_vehicles(Vissim, link_ranges)\n",
    "            time.sleep(0.1)\n",
    "        if vehicles is None:\n",
    "            continue\n",
    "\n",
    "        episode_reward = 0.0\n",
    "        step = 0\n",
    "        while True:\n",
    "            try:\n",
    "                vehicles = get_valid_vehicles(Vissim)\n",
    "                if not vehicles:\n",
    "                    break\n",
    "                ego_vehicle = None\n",
    "                ego_vehicle = next((v for v in vehicles if v.AttValue('VehType') == '630'), None)\n",
    "                if ego_vehicle is None:\n",
    "                    break\n",
    "\n",
    "                ego_num = ego_vehicle.AttValue('No')\n",
    "                # ---- obs/state 생성 ----\n",
    "                obs_dict = calculate_observations(vehicles, ego_num=ego_num)\n",
    "                current_state = get_state(vehicles, obs_dict, ego_num=ego_num, simsec=Vissim.Simulation.AttValue('SimSec'))\n",
    "                # ---- 액션 선택 및 적용 ----\n",
    "                acceleration = agent.select_action(current_state, training=False)\n",
    "                apply_action(vehicles, Vissim, acceleration)\n",
    "                Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "                next_vehicles = get_valid_vehicles(Vissim)\n",
    "                if not next_vehicles:\n",
    "                    break\n",
    "\n",
    "                next_obs = calculate_observations(next_vehicles, ego_num=ego_num)\n",
    "                next_state = get_state(next_vehicles, next_obs, ego_num=ego_num, simsec=Vissim.Simulation.AttValue('SimSec'))\n",
    "\n",
    "                # ---- 보상 계산 ----\n",
    "                reward, done = get_reward(ego_vehicle, next_obs)\n",
    "                episode_reward += reward\n",
    "\n",
    "                results.append({\n",
    "                    'Episode': episode + 1,\n",
    "                    'Step': step + 1,\n",
    "                    'Speed': ego_vehicle.AttValue('Speed'),\n",
    "                    'Position': ego_vehicle.AttValue('Pos'),\n",
    "                    'Link': ego_vehicle.AttValue('Lane')\n",
    "                })\n",
    "\n",
    "                if done or is_episode_done(ego_vehicle, next_obs):\n",
    "                    break\n",
    "                step += 1\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during evaluation step: {str(e)}\")\n",
    "                break\n",
    "            \n",
    "        print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward:.2f}\")\n",
    "    return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a39168d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LearningMonitor:\n",
    "    def __init__(self):\n",
    "        # 슬라이딩 윈도우용 메트릭 (기존 구조 유지)\n",
    "        self.metrics = defaultdict(list)\n",
    "        # step-level 로그 (원하면 나중에 쓰는 용도)\n",
    "        self.episode_data = []\n",
    "        # episode-level 요약\n",
    "        self.episode_summaries = []\n",
    "\n",
    "    def add_episode_data(self, data):\n",
    "        \"\"\"에피소드별 step 데이터 저장 + 요약치 생성\"\"\"\n",
    "        self.episode_data.append(data)\n",
    "        ep_idx = len(self.episode_summaries) + 1\n",
    "\n",
    "        if not data:\n",
    "            summary = {\n",
    "                'episode': ep_idx,\n",
    "                'length': 0,\n",
    "                'total_reward': 0.0,\n",
    "                'success': False,\n",
    "                'collision': False,\n",
    "                'crossing_step': None,\n",
    "                'mean_action': 0.0,\n",
    "                'max_action': 0.0,\n",
    "                'min_action': 0.0\n",
    "            }\n",
    "        else:\n",
    "            length = len(data)\n",
    "            total_reward = sum(step['Reward'] for step in data)\n",
    "            actions = [step['Action'] for step in data]\n",
    "\n",
    "            # 성공: Link 7-1 도달 여부\n",
    "            success = any(step['Link'] == \"7-1\" for step in data)\n",
    "\n",
    "            # 충돌: 보상 기준\n",
    "            collision = any(step['Reward'] <= -100 for step in data)\n",
    "\n",
    "            # 교차로 통과 시점 (성공 에피소드만)\n",
    "            crossing_step = None\n",
    "            if success:\n",
    "                for i, step in enumerate(data):\n",
    "                    if step['Link'] == \"7-1\":\n",
    "                        crossing_step = i + 1\n",
    "                        break\n",
    "\n",
    "            summary = {\n",
    "                'episode': ep_idx,\n",
    "                'length': length,\n",
    "                'total_reward': float(total_reward),\n",
    "                'success': bool(success),\n",
    "                'collision': bool(collision),\n",
    "                'crossing_step': crossing_step,\n",
    "                'mean_action': float(np.mean(actions)),\n",
    "                'max_action': float(np.max(actions)),\n",
    "                'min_action': float(np.min(actions))\n",
    "            }\n",
    "\n",
    "        self.episode_summaries.append(summary)\n",
    "\n",
    "    def compute_metrics(self, window_size=100):\n",
    "        \"\"\"슬라이딩 윈도우 기반 메트릭 (기존 개념 유지)\"\"\"\n",
    "        if not self.episode_summaries:\n",
    "            return {}\n",
    "\n",
    "        recent = self.episode_summaries[-window_size:]\n",
    "\n",
    "        success_rate = np.mean([s['success'] for s in recent]) if recent else 0.0\n",
    "        collision_rate = np.mean([s['collision'] for s in recent]) if recent else 0.0\n",
    "        avg_episode_length = np.mean([s['length'] for s in recent]) if recent else 0.0\n",
    "        avg_reward = np.mean([s['total_reward'] for s in recent]) if recent else 0.0\n",
    "        crossing_times = [s['crossing_step'] for s in recent if s['success'] and s['crossing_step'] is not None]\n",
    "        avg_crossing_time = np.mean(crossing_times) if crossing_times else 0.0\n",
    "\n",
    "        metrics = {\n",
    "            'success_rate': success_rate,\n",
    "            'collision_rate': collision_rate,\n",
    "            'avg_episode_length': avg_episode_length,\n",
    "            'avg_reward': avg_reward,\n",
    "            'avg_crossing_time': avg_crossing_time\n",
    "        }\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            self.metrics[k].append(v)\n",
    "        return metrics\n",
    "\n",
    "    # -----------------------------\n",
    "    # 에피소드 단위 그래프\n",
    "    # -----------------------------\n",
    "    def plot_episode_rewards(self, ma_window=50):\n",
    "        \"\"\"에피소드별 total_reward + 이동평균\"\"\"\n",
    "        if not self.episode_summaries:\n",
    "            return\n",
    "        df = pd.DataFrame(self.episode_summaries)\n",
    "        df.set_index('episode', inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index, df['total_reward'], alpha=0.4, label='Total Reward (per episode)')\n",
    "        if len(df) >= ma_window:\n",
    "            df['reward_ma'] = df['total_reward'].rolling(ma_window).mean()\n",
    "            plt.plot(df.index, df['reward_ma'], linewidth=2.0, label=f'Moving Average ({ma_window} eps)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Episode Reward & Moving Average')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_episode_lengths(self, ma_window=50):\n",
    "        \"\"\"에피소드 길이 + 이동평균\"\"\"\n",
    "        if not self.episode_summaries:\n",
    "            return\n",
    "        df = pd.DataFrame(self.episode_summaries)\n",
    "        df.set_index('episode', inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df.index, df['length'], alpha=0.4, label='Episode Length')\n",
    "        if len(df) >= ma_window:\n",
    "            df['len_ma'] = df['length'].rolling(ma_window).mean()\n",
    "            plt.plot(df.index, df['len_ma'], linewidth=2.0, label=f'Moving Average ({ma_window} eps)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.title('Episode Length & Moving Average')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_crossing_times(self, ma_window=50):\n",
    "        \"\"\"교차로 통과 시간(성공 에피소드만) + 이동평균\"\"\"\n",
    "        if not self.episode_summaries:\n",
    "            return\n",
    "        df = pd.DataFrame(self.episode_summaries)\n",
    "        df.set_index('episode', inplace=True)\n",
    "\n",
    "        # crossing_step가 있는 에피소드만\n",
    "        df_success = df[df['crossing_step'].notnull()].copy()\n",
    "        if df_success.empty:\n",
    "            print(\"No successful episodes for crossing time plot.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.scatter(df_success.index, df_success['crossing_step'],\n",
    "                    alpha=0.5, s=10, label='Crossing Step (success only)')\n",
    "        if len(df_success) >= ma_window:\n",
    "            df_success['cross_ma'] = df_success['crossing_step'].rolling(ma_window).mean()\n",
    "            plt.plot(df_success.index, df_success['cross_ma'],\n",
    "                     linewidth=2.0, label=f'Moving Average ({ma_window} eps)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Step')\n",
    "        plt.title('Crossing Time (First enter Link 7-1)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_action_trends(self, ma_window=50):\n",
    "        \"\"\"에피소드별 mean_action + 이동평균 (정책 경향성 확인용)\"\"\"\n",
    "        if not self.episode_summaries:\n",
    "            return\n",
    "        df = pd.DataFrame(self.episode_summaries)\n",
    "        df.set_index('episode', inplace=True)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(df.index, df['mean_action'], alpha=0.4, label='Mean Action per Episode')\n",
    "        if len(df) >= ma_window:\n",
    "            df['act_ma'] = df['mean_action'].rolling(ma_window).mean()\n",
    "            plt.plot(df.index, df['act_ma'], linewidth=2.0, label=f'Moving Average ({ma_window} eps)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Acceleration [m/s^2]')\n",
    "        plt.title('Policy Action Trend (Mean Acceleration)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # -----------------------------\n",
    "    # CSV 저장\n",
    "    # -----------------------------\n",
    "    def save_episode_summaries(self, filepath):\n",
    "        \"\"\"에피소드별 요약 메트릭 저장\"\"\"\n",
    "        if not self.episode_summaries:\n",
    "            return\n",
    "        df = pd.DataFrame(self.episode_summaries)\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n",
    "    def save_metrics(self, filepath):\n",
    "        \"\"\"슬라이딩 윈도우 메트릭 저장 (기존 구조 유지)\"\"\"\n",
    "        df = pd.DataFrame(self.metrics)\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cc855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 1: Reward = -235.00\n",
      "Evaluation Episode 2: Reward = 81.00\n",
      "Evaluation Episode 3: Reward = 37.00\n",
      "Evaluation Episode 4: Reward = 60.00\n",
      "Evaluation Episode 5: Reward = 76.00\n",
      "Evaluation Episode 6: Reward = 80.00\n",
      "Evaluation Episode 7: Reward = -1.00\n",
      "Evaluation Episode 8: Reward = 38.00\n",
      "Evaluation Episode 9: Reward = 22.00\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 메인 (TD3로 학습/평가)\n",
    "# =========================\n",
    "# mode=\"train\"\n",
    "def main(mode='train'):\n",
    "    # try:\n",
    "\n",
    "    Network_Path = 'C:/Users/user/2_VISSIM_network/3_IG_RL/Network/rightTurn/'\n",
    "    inpx_Name    = 'Signal intersection_v6.inpx'\n",
    "    layx_Name    = 'Signal intersection_v6.layx'\n",
    "\n",
    "    Vissim = Network(Network_Path, inpx_Name, layx_Name)\n",
    "    if Vissim is None:\n",
    "        return\n",
    "\n",
    "\n",
    "    link_ranges = {1: (10, 30), 2: (100, 150), 3: (10, 30), 4: (100, 150)} # 조정필요\n",
    "\n",
    "    agent = TD3Agent(\n",
    "        state_dim=13, \n",
    "        act_low=-3.0, \n",
    "        act_high=+3.0,\n",
    "        actor_lr=1.5e-5, \n",
    "        critic_lr=1.5e-4,\n",
    "        gamma=0.99, \n",
    "        tau=0.001,\n",
    "        policy_noise=0.2, \n",
    "        noise_clip=0.5,\n",
    "        policy_delay=2, \n",
    "        batch_size=512,\n",
    "        start_steps=3000, \n",
    "        expl_noise=0.2\n",
    "    )\n",
    "    monitor = LearningMonitor()\n",
    "    if mode == 'train':\n",
    "        n_episodes = 5000\n",
    "        max_steps  = 2000\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            try:\n",
    "                episode_reward = 0.0\n",
    "                step = 0\n",
    "                current_episode_data = []\n",
    "                print(f\"--- Episode {episode + 1} Start ---\")\n",
    "\n",
    "                vehicles = None\n",
    "                for _ in range(3):\n",
    "                    vehicles = create_vehicles(Vissim, link_ranges)\n",
    "                    \n",
    "                    if vehicles:\n",
    "                        \n",
    "                        break\n",
    "                    time.sleep(0.1)\n",
    "                if not vehicles:\n",
    "                    print(\"Failed to create vehicles. Skipping episode.\")\n",
    "                    continue\n",
    "\n",
    "                ego_vehicle = next((v for v in vehicles if v.AttValue('VehType') == '630'), None)\n",
    "                print('여긴 ego_vehicle'+str(ego_vehicle.AttValue('No')))\n",
    "                ego_num = ego_vehicle.AttValue('No')\n",
    "                # 초기 obs/state\n",
    "                current_obs = calculate_observations(vehicles, ego_num=ego_num)\n",
    "                current_state = get_state(vehicles, current_obs, ego_num=ego_num, simsec=Vissim.Simulation.AttValue('SimSec'))\n",
    "\n",
    "                while step < max_steps:\n",
    "                    try:\n",
    "                        vehicles = get_valid_vehicles(Vissim)\n",
    "                        if not vehicles:\n",
    "                            break\n",
    "\n",
    "                        ego_vehicle = next((v for v in vehicles if v.AttValue('VehType') == '630'), None)\n",
    "                        if ego_vehicle is None:\n",
    "                            break\n",
    "                        ego_num = ego_vehicle.AttValue('No')\n",
    "                        # 최신 obs/state 갱신\n",
    "                        current_obs = calculate_observations(vehicles, ego_num=ego_num)\n",
    "                        current_state = get_state(vehicles, current_obs, ego_num=ego_num, simsec=Vissim.Simulation.AttValue('SimSec'))\n",
    "\n",
    "                        # 행동 선택 및 적용\n",
    "                        acceleration = agent.select_action(current_state, training=True)\n",
    "                        apply_action(vehicles, Vissim, acceleration)\n",
    "\n",
    "                        Vissim.Simulation.RunSingleStep()\n",
    "\n",
    "                        next_vehicles = get_valid_vehicles(Vissim)\n",
    "                        if not next_vehicles:\n",
    "                            break\n",
    "\n",
    "                        next_obs = calculate_observations(next_vehicles, ego_num=ego_num)\n",
    "                        next_state = get_state(next_vehicles, next_obs, ego_num=ego_num, simsec=Vissim.Simulation.AttValue('SimSec'))\n",
    "\n",
    "                        reward, done = get_reward(ego_vehicle, next_obs)\n",
    "                        if is_episode_done(ego_vehicle, next_obs):\n",
    "                            done = True\n",
    "\n",
    "                        episode_reward += float(reward)\n",
    "                        agent.replay_buffer.push(current_state, acceleration, reward, next_state, done)\n",
    "                        agent.train_step()\n",
    "\n",
    "                        step_data = {\n",
    "                            'Step': step + 1,\n",
    "                            'Action': acceleration,\n",
    "                            'Reward': float(reward),\n",
    "                            'Speed': ego_vehicle.AttValue('Speed'),\n",
    "                            'Position': ego_vehicle.AttValue('Pos'),\n",
    "                            'Link': ego_vehicle.AttValue('Lane')\n",
    "                        }\n",
    "                        current_episode_data.append(step_data)\n",
    "\n",
    "                        if done:\n",
    "                            break\n",
    "                        step += 1\n",
    "                        time.sleep(0.05)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in step {step}: {str(e)}\")\n",
    "                        break\n",
    "\n",
    "                monitor.add_episode_data(current_episode_data)\n",
    "                print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Steps = {step}\")\n",
    "\n",
    "\n",
    "                if (episode + 1) % 100 == 0:\n",
    "                    metrics = monitor.compute_metrics(window_size=100)\n",
    "                    print(\"\\n[Sliding-Window Metrics (last 100 episodes)]\")\n",
    "                    print(f\"Success Rate: {metrics['success_rate']:.2%}\")\n",
    "                    print(f\"Collision Rate: {metrics['collision_rate']:.2%}\")\n",
    "                    print(f\"Avg Episode Length: {metrics['avg_episode_length']:.1f}\")\n",
    "                    print(f\"Avg Reward: {metrics['avg_reward']:.1f}\")\n",
    "                    print(f\"Avg Crossing Time: {metrics['avg_crossing_time']:.1f}\")\n",
    "\n",
    "                    # 에피소드 단위 그래프\n",
    "                    monitor.plot_episode_rewards(ma_window=100)\n",
    "                    monitor.plot_episode_lengths(ma_window=100)\n",
    "                    monitor.plot_crossing_times(ma_window=100)\n",
    "                    monitor.plot_action_trends(ma_window=100)\n",
    "\n",
    "                    # CSV 저장\n",
    "                    monitor.save_episode_summaries('output/v6/episode_summaries.csv')\n",
    "                    monitor.save_metrics('output/v6/training_metrics_window.csv')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in episode {episode + 1}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "            agent.save_models('models/v6/final')\n",
    "            monitor.save_episode_summaries('output/v6/final_episode_summaries.csv')\n",
    "            monitor.save_metrics('output/v6/final_training_metrics_toy6.csv')\n",
    "\n",
    "\n",
    "    elif mode == 'evaluate':\n",
    "        agent.load_models('models/v6/final')\n",
    "\n",
    "    results_df = evaluate(Vissim, agent, link_ranges)\n",
    "    results_df.to_csv('output/v6/evaluation_results_toy6.csv', index=False)\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Critical error in main: {str(e)}\")\n",
    "    # finally:\n",
    "    #     if 'Vissim' in locals():\n",
    "    #         try:\n",
    "    #             Vissim.Simulation.Stop()\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(mode='evaluate')  # 필요 시 'evaluate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954dc86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vissim_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
